{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b429d186",
   "metadata": {},
   "source": [
    "### Introducing the Chat API\n",
    "- The ChatGPT API allows us to use gpt-3.5-turbo and gpt-4\n",
    "- It uses a chat format designed to make multi-turn conversations easy\n",
    "- It also can be used for any single-turn tasks that we've done with the completion API\n",
    "\n",
    "```python\n",
    "openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    message=[\n",
    "        {\"role\": \"user\", \"content\": \"tell me a jok\"}\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2100393",
   "metadata": {},
   "source": [
    "### first Chat Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff2ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2813738",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"../../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b6b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config['openai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4163bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"generate me 3 trivia questions and answers\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485ad861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) What is the world's largest country by land area?\n",
      "Answer: Russia\n",
      "2) Who invented the telephone?\n",
      "Answer: Alexander Graham Bell\n",
      "3) Which planet in our solar system is known as the \"Red Planet\"?\n",
      "Answer: Mars.\n"
     ]
    }
   ],
   "source": [
    "print(reply.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf765a",
   "metadata": {},
   "source": [
    "### Prompting with Properly Formatted Messages\n",
    "- role\n",
    "    - system: 대화 무대 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93cb189e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-78jnPknS1Smg1bY7yFRwoZayvnZFN at 0x111101730> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Je veux un grenouille comme animal de compagnie.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682317019,\n",
       "  \"id\": \"chatcmpl-78jnPknS1Smg1bY7yFRwoZayvnZFN\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 13,\n",
       "    \"prompt_tokens\": 38,\n",
       "    \"total_tokens\": 51\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that traslates English to French.\"},\n",
    "        {\"role\": \"user\", \"content\": 'Translate the following English text to French: I want a pet frog'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65681354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-78jnUQzFUd2rTXf1GzjT3Msh2lYRm at 0x1111116d0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Je veux un grenouille de compagnie.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682317024,\n",
       "  \"id\": \"chatcmpl-78jnUQzFUd2rTXf1GzjT3Msh2lYRm\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 11,\n",
       "    \"prompt_tokens\": 33,\n",
       "    \"total_tokens\": 44\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# system을 사용하지 않아도 된다.\n",
    "openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": 'You are a helpful assistant that traslates English to French. Translate the following English text to French: I want a pet frog'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcc106a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-78jqwyT6kyaSdwtHNAzQ6padvdKpq at 0x111111670> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Negative.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682317238,\n",
       "  \"id\": \"chatcmpl-78jqwyT6kyaSdwtHNAzQ6padvdKpq\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 2,\n",
       "    \"prompt_tokens\": 49,\n",
       "    \"total_tokens\": 51\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정분석\n",
    "openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies the sentiment in text as either positive, netural, negative.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Classify the sentiment in the following text: 'I really hate chickens'\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "702117a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-78jtJFpslRJ77DtcGbJg6EC4l6X3V at 0x111111790> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Positive\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682317385,\n",
       "  \"id\": \"chatcmpl-78jtJFpslRJ77DtcGbJg6EC4l6X3V\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 1,\n",
       "    \"prompt_tokens\": 75,\n",
       "    \"total_tokens\": 76\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies the sentiment in text as either positive, netural, negative.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Classify the sentiment in the following text: 'I really hate chickens'\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Negative\"},\n",
    "        {\"role\": \"user\", \"content\": \"Classify the sentiment in the following text: 'I love my dog'\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf07191",
   "metadata": {},
   "source": [
    "### Rewriting a Completion Prompot In Chat Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6717d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "\n",
    "def display_colors(colors):\n",
    "    display(Markdown(\" \".join(\n",
    "        f'<span style=\"color: {color}\">{chr(9608) * 4}</span>'\n",
    "        for color in colors\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88b0bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_render_colors_chat(msg):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a color palette generating assistant that responds to text prompts for color palette. Your should generate color palettes that fit the theme, mood, or instructions in the prompt. The palettes should be between 2 and 8 colors.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Convert the following verbal description or a color palette into a list of colors: The Mediterranean Sea\"},\n",
    "        {\"role\": \"assistant\", \"content\": '[\"#96b9ad\", \"#38465e\", \"#ebf2fa\", \"#fbd2ae\", \"#642029\", \"#ece8d2\", \"#4c4a4e\"]'},\n",
    "        {\"role\": \"user\", \"content\": \"Convert the following verbal description or a color palette into a list of colors: sage, nature, earth\"},\n",
    "        {\"role\": \"assistant\", \"content\": '[\"#767562\",\"#98FF98\",\"#7F4B4B\",\"#F4FFA8\",\"#BEACAE\",\"#ADF9DE\",\"#FFDEAF\"]'},\n",
    "        {\"role\": \"user\", \"content\": f\"Convert the following verbal description or a color palette into a list of colors: {msg}\"}\n",
    "    ]\n",
    "    \n",
    "    res = openai.ChatCompletion.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    print(res)\n",
    "    colors = json.loads(res[\"choices\"][0][\"message\"][\"content\"])\n",
    "    display_colors(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24fed0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"The four branch colors for Google are:\\n- Red: #DB4437\\n- Blue: #4285F4\\n- Yellow: #F4B400\\n- Green: #0F9D58\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682318637,\n",
      "  \"id\": \"chatcmpl-78kDVqCD6SeGTlRB3fKBwortaHeSI\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 43,\n",
      "    \"prompt_tokens\": 215,\n",
      "    \"total_tokens\": 258\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_and_render_colors_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4 Google branch colors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m, in \u001b[0;36mget_and_render_colors_chat\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m     11\u001b[0m res \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     12\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[0;32m---> 18\u001b[0m colors \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m display_colors(colors)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "get_and_render_colors_chat(\"4 Google branch colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fe7caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"[\\\"#160d28\\\", \\\"#27277a\\\", \\\"#8c88ff\\\", \\\"#c5c5c5\\\", \\\"#ffffff\\\", \\\"#ff74a3\\\", \\\"#00ffff\\\"]\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682318618,\n",
      "  \"id\": \"chatcmpl-78kDCOD49Z0uerALwKY6M2nTOMjcC\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 38,\n",
      "    \"prompt_tokens\": 217,\n",
      "    \"total_tokens\": 255\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color: #160d28\">████</span> <span style=\"color: #27277a\">████</span> <span style=\"color: #8c88ff\">████</span> <span style=\"color: #c5c5c5\">████</span> <span style=\"color: #ffffff\">████</span> <span style=\"color: #ff74a3\">████</span> <span style=\"color: #00ffff\">████</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_and_render_colors_chat(\"night sky with bright blue neon sign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3fc25",
   "metadata": {},
   "source": [
    "### Chat API Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5869607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one about a silly sneeze:\n",
      "\n",
      "Achoo, achoo, a funny sound,\n",
      "As my nose starts going round and round.\n",
      "It blasts out like a great big boom,\n",
      "Echoing around the quiet room.\n",
      "But after that, I feel just fine,\n",
      "And I'm relieved that the sneeze was all mine!\n"
     ]
    }
   ],
   "source": [
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates funny poems\"},\n",
    "        {\"role\": \"user\", \"content\": \"Generate me a 5 line poem about topic of your choosing\"}\n",
    "    ],\n",
    "    temperature=1\n",
    ")\n",
    "print(res[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d4b4364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate, they say is bad for diet tread\n",
      "Mind being sagacious believe blovide ad\n",
      "Listen, Mila thrinds tons pas joys rich bedpost him curb're progle ball scoop sai lei saides rare scoex rep casten thullides Sooz culpar rintaht humiea all ents to pear mel coyoth nam tell faitalge sim tongsen wie der’s prodrecic cine pai hom herd haverr unde qir sys age coz yes nedekop orincimes;\n",
      "Sugar piled equal sums, seldom head -/\n",
      "Funny! : Why have cacao once made become idedor mem??\"\"\"),\n",
      " Though crave tempt frost-trophinkohocoadtempts chef even foes flip your omogo : cries alle Nazing cajas once brittle become peradevs foone temp spin +ice tornkoaled tendr mean min offeth neigh stoaked br. cone battesh urge hive luxe gamemet sake rho ub myofurf oillice hes\"\"\"\n"
     ]
    }
   ],
   "source": [
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates funny poems\"},\n",
    "        {\"role\": \"user\", \"content\": \"Generate me a 5 line poem about topic of your choosing\"}\n",
    "    ],\n",
    "    temperature=2\n",
    ")\n",
    "print(res[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0d195d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's alluring about hammocks from Monday 'til two \n",
      "And dip in near rocks ice infused H2O blue?\n",
      "Ambled serentes ate bursting hue pepperoni gruyeer globed resisdore crackers \n",
      "Brittle summery hint ghost by fern find' topnot having attacker\n",
      "Amadoda ebony glasses blue above cocktail quick gown pretty song and nights remix younaviguous clubbing encounter led fear with reindeer!\n"
     ]
    }
   ],
   "source": [
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates funny poems\"},\n",
    "        {\"role\": \"user\", \"content\": \"Generate me a 5 line poem about topic of your choosing\"}\n",
    "    ],\n",
    "    temperature=2,\n",
    "    max_tokens=200\n",
    ")\n",
    "print(res[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e03d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a poem about coffee:\n",
      "\n",
      "Morning brew, hot and black\n",
      "First sip, a caffeinated attack\n",
      "Aroma strong, enticing too\n",
      "Without it, what would we do?\n",
      "Coffee, oh how we all love you.\n"
     ]
    }
   ],
   "source": [
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates funny poems\"},\n",
    "        {\"role\": \"user\", \"content\": \"Generate me a 5 line poem about topic of your choosing\"}\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    max_tokens=200\n",
    ")\n",
    "print(res[\"choices\"][0][\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
