{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc00507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b064b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"../../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa4f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config[\"openai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2acc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ccb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_print(dictionary):\n",
    "    for key, value in dictionary.items():\n",
    "        wrapped_text = \"\\n\".join(textwrap.wrap(value, width=120))\n",
    "        print(f\"{key}:\")\n",
    "        print(wrapped_text)\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ec312",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "- A value from 0-2, though most often between 0 and 1\n",
    "- It's default is 1\n",
    "- Controls the randomness of the output. Higher values are more random, lower values are more deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44859344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0:\n",
      "pizza. I love the combination of the doughy crust, the tangy tomato sauce, and the melty cheese. I also like to add\n",
      "different toppings like pepperoni, mushrooms, and olives. Pizza is a great meal for any occasion, whether it's a casual\n",
      "night in or a special celebration.\n",
      "====================\n",
      "Temperature 0.5:\n",
      "pizza. I love the combination of cheese, sauce, and a variety of toppings. The crust is usually my favorite part, and I\n",
      "like to get creative with the toppings. I always get a thin crust pizza because I like the crunchy texture. Pizza is\n",
      "great for any occasion, from a quick snack to a full meal.\n",
      "====================\n",
      "Temperature 1:\n",
      "chicken nuggets.  Chicken nuggets are crispy on the outside and juicy on the inside. They are also perfect for dipping\n",
      "in sauces like honey mustard, barbeque, or ranch. They make for a great snack or meal when served with a side of fries\n",
      "or a salad.\n",
      "====================\n",
      "Temperature 1.5:\n",
      "pizza.\n",
      "====================\n",
      "Temperature 2:\n",
      "watercress dried beef soup. It's a traditional Jingshi soup in Southeast China characterize mainly mainland birds\n",
      "appetite velvet/monster drum access refreshing, suitable fats smoked ferringity vivid herbal undercaries and hint spoon\n",
      "gravy lay favor white bamboo comphenece village presenting better enjoy eastern reputation hot dipping mark even pure\n",
      "touch nose steam blowing concept's like Jiuling Town pool carpet ca\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "nice_print(\n",
    "    {\n",
    "       f\"Temperature {temperature}\": openai.Completion.create(model=\"text-davinci-003\", prompt=\"\"\"My favorite food is\"\"\".strip(), max_tokens=75, temperature=temperature)\\\n",
    "       .choices[0][\"text\"] \\\n",
    "       .strip() \\\n",
    "       for temperature in [0, 0.5, 1, 1.5, 2]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b2065",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8fa82",
   "metadata": {},
   "source": [
    "## Top P\n",
    "- An alternative to sampling with temperature, called uncleus sampling\n",
    "- It's default value is 1\n",
    "- Like temperature, top p alters the \"creativity\" and randomness of the output\n",
    "- Top p controls the set of possible words/tokens that the model can choose from\n",
    "- It restricts the candidate words to the smallest set whose cumulative probability is greater than or equal to a given threshold, \"p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec3e46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top p 1:\n",
      "My favorite food is pizza. I love pizza because it's so versatile. You can top it with almost anything and make it\n",
      "different every time. Whether its loaded with veggies, covered with pepperoni, or smothered in cheese, pizza always hits\n",
      "the spot.\n",
      "====================\n",
      "Top p 0.5:\n",
      "My favorite food is pizza. I love pizza because it is delicious and there are so many different varieties to choose\n",
      "from. From classic pepperoni to creative combinations like barbecue chicken and pineapple, there is something for\n",
      "everyone. Plus, it's easy to make and share with friends and family.\n",
      "====================\n",
      "Top p 0:\n",
      "My favorite food is pizza. I love the combination of the doughy crust, the tangy tomato sauce, and the melty cheese. I\n",
      "also like to add different toppings like pepperoni, mushrooms, and olives. Pizza is a great meal for any occasion,\n",
      "whether it's a casual night in or a special celebration.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "nice_print(\n",
    "    {\n",
    "        f\"Top p {top_p}\": openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"\"\"My favorite food is\"\"\".strip(),\n",
    "            max_tokens=75,\n",
    "            echo=True,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        .choices[0][\"text\"]\n",
    "        .strip()\n",
    "        for top_p in [1, 0.5, 0]\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d5496",
   "metadata": {},
   "source": [
    "top p를 늘리고, temperature를 높이면 문서는 하나만 수행할 것을 권장함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5ae65",
   "metadata": {},
   "source": [
    "---\n",
    "## Frequency Penalty\n",
    "- A number from -2 to 2\n",
    "- Defaults to 0\n",
    "- Positive values penalize new tokens based on their existing frequency in the text so far, **decreasing the model's likelihood to repeat the same line verbatim**.\n",
    "- If you want to reduce repetitive samples, try a penalty from 0.1 - 1\n",
    "- To strongly suppress repentition, you can increase it further BUT this can lead to bad quality outputs\n",
    "- Negarive values increase the likelyhood of repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817fb4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Penalty -2:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "====================\n",
      "Frequency Penalty -1:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium,, Aluminum, Silicon, Phosphorus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "====================\n",
      "Frequency Penalty 0:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon and Phosphorus\n",
      "====================\n",
      "Frequency Penalty 1:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon and Phosphorus.\n",
      "====================\n",
      "Frequency Penalty 2:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen , Fluorine , Neon ,\n",
      "Sodium , Magnesium , Aluminum\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "nice_print(\n",
    "{\n",
    "    f\"Frequency Penalty {x}\": openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=\"\"\"The first 15 elements are Hydrogen, Helium,\"\"\".strip(),\n",
    "        max_tokens=200,\n",
    "        echo=True,\n",
    "        frequency_penalty=x\n",
    "    )\n",
    "    .choices[0][\"text\"]\n",
    "    .strip()\n",
    "    for x in [-2, -1, 0, 1, 2]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7991f00",
   "metadata": {},
   "source": [
    "---\n",
    "## Presence Penalty\n",
    "- A number from -2 to 2\n",
    "- Defaults to 0\n",
    "- Positive values penalize new tokens based on whether they appear in the text so far, **increasing the model's likelihood to talk about new topics**.\n",
    "- Presence panalty is a **one-off additive contiribution** that applies to all tokens that have been sampled at least once\n",
    "- Frequency penalty is a contribution that is **proportional** to how often a particular token hash already been sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8afdd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty -2:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon, and Phosphorus.\n",
      "====================\n",
      "Presence Penalty -1:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon and Phosphorus.\n",
      "====================\n",
      "Presence Penalty 0:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon, Phosphorus.\n",
      "====================\n",
      "Presence Penalty 1:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon and Phosphorus.\n",
      "====================\n",
      "Presence Penalty 2:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminium, Silicon, and Phosphorus.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "nice_print(\n",
    "{\n",
    "    f\"Presence Penalty {x}\": openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=\"\"\"The first 15 elements are Hydrogen, Helium,\"\"\".strip(),\n",
    "        max_tokens=200,\n",
    "        echo=True,\n",
    "        presence_penalty=x\n",
    "    )\n",
    "    .choices[0][\"text\"]\n",
    "    .strip()\n",
    "    for x in [-2, -1, 0, 1, 2]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bcc60",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adcb5276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-78fqyxrMBXinVjllmh3UQXojCK3Rs at 0x10acd76b0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nFog on the streets, a heavy morning chill.\\nThe humid air, making me ill.  \\nCommuters hurry to catch their bus,\\nForgetting to marvel at the morning fog's fuss.\\n\\nDriving along so it can be seen,\\nThe skies are grey and at times mean.\\nPassing through the misty street lights,\\nA hazy world creates a twilight.\\n\\nMuffled sounds of car horns blaring.\\nThe distant siren of an ambulance caring.\\nWrapped in the fog a slow drive ahead,\\nSteady and cool, until it\\u2019s time to leave our bed.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682301864,\n",
       "  \"id\": \"cmpl-78fqyxrMBXinVjllmh3UQXojCK3Rs\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 134,\n",
       "    \"prompt_tokens\": 10,\n",
       "    \"total_tokens\": 144\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"write me a poem about foggy morning commutes\",\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a97c590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "When the morning fog sets in, it's a destitute scene\n",
      "As I drive to work I feel like I'm in a dream\n",
      "I can't see the road, not even a dim light\n",
      "Visibility is so poor, it's like I'm in a fight\n",
      "\n",
      "Driving slowly, I move ahead\n",
      "The humidity on my glasses, it's like I've been mislead\n",
      "The limited view that I have, it's a dire sight\n",
      "Wondering how much longer until morning light\n",
      "\n",
      "The fog is ever so thick, giving an eerie feeling\n",
      "But I'm no stranger to this, so there's no need for squealing\n",
      "As I drove I listened to the radio play a song\n",
      "Helped me see that I could go the distance and be strong\n",
      "\n",
      "When I made it to work, fog slowly lifting away\n",
      "My morning commute had finally come to an end of day"
     ]
    }
   ],
   "source": [
    "for data in openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"write me a poem about foggy morning commutes\",\n",
    "    max_tokens=300,\n",
    "    stream=True\n",
    "):\n",
    "    print(data.choices[0].text, end=\"\", flust=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
